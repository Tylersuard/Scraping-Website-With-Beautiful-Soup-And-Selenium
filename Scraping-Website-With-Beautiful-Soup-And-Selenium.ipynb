{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TC_scrape OOP with Inserts",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "orIwcUGTa0ls"
      },
      "source": [
        "#install Selenium, this is for Colab only\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ57hEQ24YcR"
      },
      "source": [
        "#Set up Selenium\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "selenium_webdriver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "selenium_webdriver.get(\"https://www.google.com\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSNIRNkO1cf"
      },
      "source": [
        "#Set up Beautiful Soup\n",
        "\n",
        "import urllib\n",
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import re\n",
        "\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdtUQ79yrtU0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSaAhpUHasB"
      },
      "source": [
        "#Create end trigger for brand-year page\n",
        "\n",
        "brand_url_pass_through = urllib.parse.quote('https://www.website.com/brand/example_brand', safe='://')\n",
        "uClient = uReq(brand_url_pass_through)\n",
        "page_html = uClient.read()\n",
        "uClient.close()\n",
        "\n",
        "#find all 'a' tags\n",
        "global brand_page_soup\n",
        "brand_page_soup = soup(page_html, \"html.parser\")\n",
        "brand_year_list = brand_page_soup.find_all('a')\n",
        "\n",
        "print(brand_year_list)\n",
        "\n",
        "global brand_year_list_start\n",
        "brand_year_list_start = 25\n",
        "global end_trigger\n",
        "end_trigger = brand_year_list[30]\n",
        "print(end_trigger)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udViSKn9Yi5r"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "#download list of all brand-year URLs\n",
        "\n",
        "#CURRENTLY SET TO: category\n",
        "\n",
        "brands_url = \"https://www.website.com/brands\"\n",
        "\n",
        "  #Important!  Any time you get a page for Bs4 using uClient, navigate to that same page using Selenium too!\n",
        "  #otherwise things will get confused.\n",
        "\n",
        "selenium_webdriver.get(brands_url)\n",
        "\n",
        "uClient = uReq(brands_url)\n",
        "page_html = uClient.read()\n",
        "uClient.close()\n",
        "\n",
        "global brands_page_soup\n",
        "brands_page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  \n",
        "global brands_list\n",
        "brands_list = brands_page_soup.find_all('a')\n",
        "#print(brands_list)\n",
        "print(brands_list[120:-68])\n",
        "\n",
        "\n",
        "#CHANGE THIS FOR EVERY Category:\n",
        "brands_list = brands_list[120:-68]\n",
        "\n",
        "global brand_urls_list\n",
        "brand_urls_list = []\n",
        "\n",
        "for i in range(len(brands_list)):\n",
        "  brand_url = \"https://www.website.com\" + brands_list[i]['href']\n",
        "  brand_urls_list.append(brand_url)\n",
        "\n",
        "print(brand_urls_list)\n",
        "\n",
        "\n",
        "global all_brand_year_urls_list\n",
        "all_brand_year_urls_list = []\n",
        "\n",
        "for i in range(len(brand_urls_list)):\n",
        "  open_brand = brand_urls_list[i]\n",
        "  selenium_webdriver.get(open_brand)\n",
        "\n",
        "#open brand page in bs4\n",
        "  brand_url_pass_through = urllib.parse.quote(open_brand, safe='://')\n",
        "  uClient = uReq(brand_url_pass_through)\n",
        "  page_html = uClient.read()\n",
        "  uClient.close()\n",
        "\n",
        "#find all 'a' tags\n",
        "  global brand_page_soup\n",
        "  brand_page_soup = soup(page_html, \"html.parser\")\n",
        "  brand_year_list = brand_page_soup.find_all('a')\n",
        "  brand_year_end = brand_year_list.index(end_trigger)\n",
        "  \n",
        "  brand_year_list = brand_year_list[brand_year_list_start:brand_year_end]\n",
        "  \n",
        "  for i in range(len(brand_year_list)):\n",
        "    byurl = brand_year_list[i]['href']\n",
        "    full_byurl = \"https://www.website.com\" + byurl\n",
        "    all_brand_year_urls_list.append(full_byurl)\n",
        "    print(full_byurl)\n",
        "\n",
        "\n",
        "  #directory = brand_url[35:]\n",
        "  #print(directory)\n",
        "  #os.makedirs(directory)\n",
        "  #global current_brand\n",
        "  #current_brand = directory[18:]\n",
        "  #print(current_brand)\n",
        "\n",
        "\n",
        "#create folders for each brand\n",
        "#iterate through list\n",
        "#set variable 'current_brand'\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyjzEwIfXvIO"
      },
      "source": [
        "\"\"\"\n",
        "#save brand-year URLs list to .csv file\n",
        "\n",
        "import csv     \n",
        "\n",
        "with open('category_all_brand_year_urls.csv', 'w') as f: \n",
        "\n",
        "  for b_url in all_brand_year_urls_list:\n",
        "    write = csv.writer(f) \n",
        "    #addline = all_brand_year_urls_list[i]\n",
        "    write.writerow([b_url]) \n",
        "    #print(all_brand_year_urls_list[i])\n",
        "  f.close()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssfKvV12UHaY"
      },
      "source": [
        "#load brand-year URLs list from .csv file\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "brand_year_urls_all_list = pd.read_csv(\"/content/all_brand_year_urls.csv\", header=None)\n",
        "\n",
        "print(brand_year_urls_all_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4arAeiF1wJvs"
      },
      "source": [
        "brand_year_urls_all_list = brand_year_urls_all_list[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zN77iGwsiU"
      },
      "source": [
        "print(brand_year_urls_all_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXqLxp8ZYNJ9"
      },
      "source": [
        "#After this, just make a similar code that scrapes the inserts and the parallels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLyG-azUph8U"
      },
      "source": [
        "#open specific brand\n",
        "#roll all the brand-year options into a list\n",
        "#create a folder for each brand-year\n",
        "#enter into first brand-year fodler\n",
        "\n",
        "\"\"\"\n",
        "def create_list_of_brand_years(brand_url_input):\n",
        "  #Important!  Any time you get a page for Bs4 using uClient, navigate to that same page using Selenium too!\n",
        "  #otherwise things will get confused.\n",
        "  selenium_webdriver.get(brand_url_input)\n",
        "\n",
        "\n",
        "  brand_url_pass_through = urllib.parse.quote(brand_url_input, safe='://')\n",
        "  uClient = uReq(brand_url_pass_through)\n",
        "  #uClient = uReq(\"https://www.website.com/ViewAll.cfm/sp/category/brand/example_brand\")\n",
        "  page_html = uClient.read()\n",
        "\n",
        "  uClient.close()\n",
        "\n",
        "  global brand_page_soup\n",
        "  \n",
        "  brand_page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  \n",
        "  global brand_year_list\n",
        "\n",
        "  brand_year_list_raw = brand_page_soup.find_all('a')\n",
        "\n",
        "  print(brand_year_list_raw)\n",
        "\n",
        "  brand_year_list_links = []\n",
        "\n",
        "#Ok, important thing to note\n",
        "#When you make a list of all the brand-years in a set, you need to first:\n",
        "#1. get all <a> tags using bs4 (returns in list form)\n",
        "#2. remove the first x number of items from the list\n",
        "#3. find some standardized way to remove the end items from the list, like index by href value\n",
        "\n",
        "#  for i in range(len(brand_year_list_raw)):\n",
        "#    brand_year = brand_year_list_raw[i]['href']\n",
        "#    brand_year_list_links.append(brand_year)\n",
        "\n",
        "#  print(brand_year_list_links)\n",
        "  #print(brands_list[86:-67])\n",
        "\n",
        "  #brand_list = brand_list[86:-67]\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H4UL1euFpq2"
      },
      "source": [
        "#Create end trigger for every parallels and inserts page\n",
        "\n",
        "inserts_url_pass_through = urllib.parse.quote('https://www.website.com/example_brand', safe='://')\n",
        "uClient = uReq(inserts_url_pass_through)\n",
        "page_html = uClient.read()\n",
        "uClient.close()\n",
        "\n",
        "#find all 'a' tags\n",
        "global inserts_page_soup\n",
        "inserts_page_soup = soup(page_html, \"html.parser\")\n",
        "inserts_links_list = inserts_page_soup.find_all('a')\n",
        "\n",
        "#print(inserts_links_list)\n",
        "\n",
        "global inserts_list_start\n",
        "inserts_list_start = 95\n",
        "print(inserts_links_list[85])\n",
        "global inserts_list_end_trigger\n",
        "inserts_list_end_trigger = inserts_links_list[-68]\n",
        "print(inserts_list_end_trigger)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvTOVth_Oweh"
      },
      "source": [
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "global brand_year_urls_all_list\n",
        "brand_year_urls_all_list = pd.read_csv(\"/content/all_brand_year_urls.csv\", header=None)\n",
        "\n",
        "\n",
        "\n",
        "#brand_year_urls_all_list = brand_year_urls_all_list[2:]\n",
        "print(brand_year_urls_all_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn5pvImjbXJR"
      },
      "source": [
        "#open the info set and find the URL of the page showing inserts and parallels (DONE!)\n",
        "#open the first parallel/insert set\n",
        "#Create an exception!  There may be individual infos rather than full sets of parallels and inserts!\n",
        "#open the first info\n",
        "#scrape the first info\n",
        "#click next\n",
        "#scrape the second info\n",
        "#repeat\n",
        "\n",
        "import csv     \n",
        "\n",
        "def find_url_for_inserts_and_parallels(info_set_location):\n",
        "  selenium_webdriver.get(info_set_location)\n",
        "  inserts_and_parallels_link = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[1]/div[1]/div[2]/a[6]')\n",
        "  gray_or_not = inserts_and_parallels_link.get_attribute('class')\n",
        "\n",
        "  if gray_or_not == 'grayed':\n",
        "    print(gray_or_not)\n",
        "    print(\"no parallels\")\n",
        "    return None\n",
        "\n",
        "  else:\n",
        "    print(\"got parallels link\")\n",
        "    global inserts_and_parallels_page_url\n",
        "    inserts_and_parallels_page_url = inserts_and_parallels_link.get_attribute('href')\n",
        "    #print(inserts_and_parallels_page_url)  \n",
        "    return inserts_and_parallels_page_url\n",
        "\n",
        "#find_url_for_inserts_and_parallels('https://www.website.com/brands/example_brand.html')\n",
        "#^this works don't touch!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_list_of_inserts_and_parallels_urls_for_set(inserts_and_parallels_page_url):\n",
        "  if inserts_and_parallels_page_url is not None:\n",
        "\n",
        "    selenium_webdriver.get(inserts_and_parallels_page_url)\n",
        "\n",
        "    uClient = uReq(inserts_and_parallels_page_url)\n",
        "    page_html = uClient.read()\n",
        "    uClient.close()\n",
        "\n",
        "    global inserts_and_parallels_listing_page_soup\n",
        "    inserts_and_parallels_listing_page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "    \n",
        "    global inserts_and_parallels_list\n",
        "    inserts_and_parallels_list = inserts_and_parallels_listing_page_soup.find_all('a')\n",
        "    print(inserts_and_parallels_list)\n",
        "    print(inserts_and_parallels_list[75:-68])\n",
        "    inserts_and_parallels_list = inserts_and_parallels_list[75:=68]\n",
        "\n",
        "    for i in range(len(inserts_and_parallels_list)):\n",
        "      i_p_url = \"https://www.website.com\" + inserts_and_parallels_list[i]['href']\n",
        "      brand_urls_list.append(i_p_url)\n",
        "      print(i_p_url)\n",
        "\n",
        "      with open('baseball_all_inserts_and_parallels_urls.csv', 'a+') as f:\n",
        "      write = csv.writer(f) \n",
        "      write.writerow(i_p_url) \n",
        "      f.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwgL-nT5O33l"
      },
      "source": [
        "for brandyear in brand_year_urls_all_list:\n",
        "  find_url_for_inserts_and_parallels(brandyear)\n",
        "  find_list_of_inserts_and_parallels_urls_for_set(inserts_and_parallels_page_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_BjYYcvOm8n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guzuFXwjDLvb"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "#download list of all brand-year URLs\n",
        "\n",
        "#CURRENTLY SET TO: Category\n",
        "\n",
        "brands_url = \"https://www.website.com/brands\"\n",
        "\n",
        "  #Important!  Any time you get a page for Bs4 using uClient, navigate to that same page using Selenium too!\n",
        "  #otherwise things will get confused.\n",
        "\n",
        "selenium_webdriver.get(brands_url)\n",
        "\n",
        "uClient = uReq(brands_url)\n",
        "page_html = uClient.read()\n",
        "uClient.close()\n",
        "\n",
        "global brands_page_soup\n",
        "brands_page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  \n",
        "global brands_list\n",
        "brands_list = brands_page_soup.find_all('a')\n",
        "#print(brands_list)\n",
        "print(brands_list[120:-68])\n",
        "\n",
        "\n",
        "#CHANGE THIS FOR EVERY Category:\n",
        "brands_list = brands_list[120:-68]\n",
        "\n",
        "global brand_urls_list\n",
        "brand_urls_list = []\n",
        "\n",
        "for i in range(len(brands_list)):\n",
        "  brand_url = \"https://www.website.com\" + brands_list[i]['href']\n",
        "  brand_urls_list.append(brand_url)\n",
        "\n",
        "print(brand_urls_list)\n",
        "\n",
        "\n",
        "global all_brand_year_urls_list\n",
        "all_brand_year_urls_list = []\n",
        "\n",
        "for i in range(len(brand_urls_list)):\n",
        "  open_brand = brand_urls_list[i]\n",
        "  selenium_webdriver.get(open_brand)\n",
        "\n",
        "#open brand page in bs4\n",
        "  brand_url_pass_through = urllib.parse.quote(open_brand, safe='://')\n",
        "  uClient = uReq(brand_url_pass_through)\n",
        "  page_html = uClient.read()\n",
        "  uClient.close()\n",
        "\n",
        "#find all 'a' tags\n",
        "  global brand_page_soup\n",
        "  brand_page_soup = soup(page_html, \"html.parser\")\n",
        "  brand_year_list = brand_page_soup.find_all('a')\n",
        "  brand_year_end = brand_year_list.index(end_trigger)\n",
        "  \n",
        "  brand_year_list = brand_year_list[brand_year_list_start:brand_year_end]\n",
        "  \n",
        "  for i in range(len(brand_year_list)):\n",
        "    byurl = brand_year_list[i]['href']\n",
        "    full_byurl = \"https://www.website.com\" + byurl\n",
        "    all_brand_year_urls_list.append(full_byurl)\n",
        "    print(full_byurl)\n",
        "\n",
        "\n",
        "  #directory = brand_url[35:]\n",
        "  #print(directory)\n",
        "  #os.makedirs(directory)\n",
        "  #global current_brand\n",
        "  #current_brand = directory[18:]\n",
        "  #print(current_brand)\n",
        "\n",
        "\n",
        "#create folders for each brand\n",
        "#iterate through list\n",
        "#set variable 'current_brand'\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O3lfeixDLuB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiHnhIRSToY"
      },
      "source": [
        "# scrape the parallels ONLY if input from find_url_for_inserts_and_parallels is not None."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZndSy3jHSbRv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34PluP2sO056"
      },
      "source": [
        "#open the info set and find the URL of the parallel/insert link\n",
        "def find_url_for_first_info_info_page_in_set(info_set_location):\n",
        "  selenium_webdriver.get(info_set_location)\n",
        "  first_info_number_in_set = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[1]/div[2]/table/tbody/tr[1]/td[4]/a')\n",
        "  first_info_page_url = first_info_number_in_set.get_attribute('href')\n",
        "  #print(first_info_page_url)\n",
        "  return first_info_page_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFT0c0jkPASF"
      },
      "source": [
        "#open the first info's URL and print the title\n",
        "def open_first_info_info_page(first_info_location):\n",
        "  info_url = first_info_location\n",
        "\n",
        "  #Important!  Any time you get a page for Bs4 using uClient, navigate to that same page using Selenium too!\n",
        "  #otherwise things will get confused.\n",
        "  selenium_webdriver.get(info_url)\n",
        "\n",
        "  uClient = uReq(urllib.parse.quote(info_url, safe='://'))\n",
        "  page_html = uClient.read()\n",
        "\n",
        "  uClient.close()\n",
        "\n",
        "  global page_soup\n",
        "  \n",
        "  page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  global title\n",
        "  title = page_soup.h4.text.strip()\n",
        "\n",
        "  print(\"we are now at \" + info_url)\n",
        "  print(\"info title = \" + title)\n",
        "  return(title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYRtxUScPDnF"
      },
      "source": [
        "#Get the info info (make sure to add the SN99 or the other thing!)\n",
        "def scrape_info_info(info_title):\n",
        "  page_blocks = page_soup.findAll(\"div\", {\"class\":\"block1\"})\n",
        "\n",
        "  description_block = page_blocks[0]\n",
        "\n",
        "  desc_block_text = description_block.text.strip()\n",
        "\n",
        "  global desc_block_splitln\n",
        "\n",
        "  desc_block_splitln = desc_block_text.splitlines()\n",
        "\n",
        "  global title_splitln\n",
        "  title_splitln = info_title.splitlines()\n",
        "\n",
        "  print(info_title)\n",
        "  #print(title_splitln)\n",
        "  #print(desc_block_splitln)\n",
        "\n",
        "  return title_splitln\n",
        "  return desc_block_splitln"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6phRzo7POqc"
      },
      "source": [
        "def parse_info_info_to_txt(desc_block_splitlines,title_splitlines):\n",
        "  category = desc_block_splitlines[1]\n",
        "  year = desc_block_splitlines[3]\n",
        "  year = str(year)\n",
        "  info_set = desc_block_splitlines[8]\n",
        "  info_set = str(info_set)\n",
        "  if \"Total infos\" in info_set:\n",
        "    info_set = desc_block_splitlines[7]\n",
        "    info_set = str(info_set)\n",
        "\n",
        "  #print(desc_block_splitlines)\n",
        "  print(info_set)\n",
        "  info_number = title[:4]\n",
        "  info_number = str(info_number)\n",
        "  person_name = title_splitlines[0][4:]\n",
        "  id_number = title_splitlines[1]\n",
        "  group = title_splitlines[2]\n",
        "\n",
        "  person_name = re.sub('[-]+','',person_name, flags=re.UNICODE)\n",
        "  person_name = person_name.strip()\n",
        "  \n",
        "  group = re.sub('[-]+','',group, flags=re.UNICODE)\n",
        "  group = group.strip()\n",
        "\n",
        "  global category_filename\n",
        "  category_filename = category.replace(\" \", \"\")\n",
        "\n",
        "  global year_filename\n",
        "  year_filename = year.replace(\" \", \"\")\n",
        "\n",
        "  global info_set_filename\n",
        "  info_set_filename = info_set.replace(\" \", \"\")\n",
        "\n",
        "  global info_number_filename\n",
        "  info_number_filename = info_number.replace(\"#\", \"\")\n",
        "  info_number_filename = info_number_filename.replace(\" \", \"\")\n",
        "\n",
        "  global person_name_filename\n",
        "  person_name_filename = person_name.replace(\" \", \"\")\n",
        "  person_name_filename = person_name_filename.replace(\"/\", \"_\")\n",
        "\n",
        "  global id_number_filename\n",
        "  id_number_filename = id_number.replace(\" \",\"\")\n",
        "\n",
        "  global group_filename\n",
        "  group_filename = group.replace(\" \", \"\")\n",
        "\n",
        "\n",
        "\n",
        "  info_url = selenium_webdriver.current_url\n",
        "\n",
        "\n",
        "\n",
        "  #filename = category_filename+\"_\"+year_filename+\"_\"+info_set_filename+\"_\"+info_number_filename+\"_\"+person_name_filename+\"_\"+group_filename+\".txt\"\n",
        "\n",
        "\n",
        "  filename = str(info_set_filename+\"_\"+person_name_filename+\".txt\")\n",
        "  #print(filename)\n",
        "\n",
        "  save_dir = \"/\" + category_filename + \"/\" +info_set_filename + \"/base/\"\n",
        "\n",
        "  try:\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  print(\"id number:\")\n",
        "  print(id_number)\n",
        "\n",
        "  if id_number != ' ':\n",
        "    write_info = category+\"\\n\"+year+\"\\n\"+info_set+\"\\n\"+info_number+\"\\n\"+person_name+\"\\n\"+id_number+\"\\n\"+group+\"\\n\"+info_url\n",
        "    filepath = os.path.join(save_dir,filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "      f.write(write_info)\n",
        "      f.close\n",
        "\n",
        "  else:\n",
        "    write_info = category+\"\\n\"+year+\"\\n\"+info_set+\"\\n\"+info_number+\"\\n\"+person_name+\"\\n\"+group+\"\\n\"+info_url\n",
        "    filepath = os.path.join(save_dir,filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "      f.write(write_info)\n",
        "      f.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5hFMwkf1kAk"
      },
      "source": [
        "##THIS IS FOR TESTING THE IMAGE DOWNLOADING \n",
        "\n",
        "selenium_webdriver.get(\"https://www.website.com/Viewinfo.cfm/sid/73934/cid/5299406?PageIndex=1\")\n",
        "\n",
        "#info_image_front = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[1]/div/div/a/img')\n",
        "info_image_front = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[1]/div/img')\n",
        "info_image_front_link = info_image_front.get_attribute('src')\n",
        "\n",
        "print(info_image_front_link)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtsKNMpfPTt1"
      },
      "source": [
        "#download the images\n",
        "#Ok so for some reason this code only works when both info images are there!\n",
        "def download_info_images_from_current_page():\n",
        "\n",
        "  try:\n",
        "    info_image_front = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[1]/div/div/a/img')\n",
        "  except:\n",
        "    info_image_front = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[1]/div/img')\n",
        "  \n",
        "  \n",
        "  info_image_front_link = info_image_front.get_attribute('src')\n",
        "\n",
        "  print(info_image_front_link)\n",
        "  \n",
        "    \n",
        "  try:\n",
        "    info_image_back = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[2]/div[2]/div/a/img')\n",
        "  except:\n",
        "    info_image_back = selenium_webdriver.find_element_by_xpath('/html/body/div/div[3]/div[2]/div[1]/table[2]/tbody/tr/td/table/tbody/tr/td/div/div[2]/div[2]/img')\n",
        "\n",
        "\n",
        "  info_image_back_link = info_image_back.get_attribute('src')\n",
        "    \n",
        "  print(info_image_back_link)\n",
        "\n",
        "\n",
        "  if info_image_front_link != \"https://www.website.com/Images/blank.gif\":\n",
        "    #get just the filename from the URL and save the image as that filename\n",
        "    parsed_link_front = urlparse(info_image_front_link)\n",
        "    #print(parsed_link_front)\n",
        "    info_front_image_filename = os.path.basename(parsed_link_front.path)\n",
        "    info_front_image_full_name = info_set_filename+\"_\"+person_name_filename+info_front_image_filename\n",
        "    \n",
        "    save_dir = \"/\" + category_filename + \"/\" +info_set_filename + \"/base/\"\n",
        "    front_filepath = os.path.join(save_dir,info_front_image_full_name)\n",
        "\n",
        "\n",
        "    try:\n",
        "      os.makedirs(save_dir)\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    print(front_filepath)\n",
        "\n",
        "    info_image_front_link = urllib.parse.quote(info_image_front_link, safe='://')\n",
        "    urllib.request.urlretrieve(info_image_front_link, front_filepath)\n",
        "\n",
        "\n",
        "  if info_image_back_link != \"https://www.website.com/Images/blank2.gif\":\n",
        "    #get just the filename from the URL and save the image as that filename\n",
        "    parsed_link_back = urlparse(info_image_back_link)\n",
        "    info_back_image_filename = os.path.basename(parsed_link_back.path)\n",
        "    info_back_image_full_name = info_set_filename+\"_\"+person_name_filename+info_back_image_filename\n",
        "    print(info_back_image_full_name)\n",
        "\n",
        "    save_dir = \"/\" + category_filename + \"/\" +info_set_filename + \"/base/\"\n",
        "    back_filepath = os.path.join(save_dir,info_back_image_full_name)\n",
        "\n",
        "    try:\n",
        "      os.makedirs(save_dir)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    print(back_filepath)\n",
        "\n",
        "    info_image_back_link = urllib.parse.quote(info_image_back_link, safe='://')\n",
        "    urllib.request.urlretrieve(info_image_back_link, back_filepath)\n",
        "\n",
        "\n",
        "#page_blocks = page_soup.findAll(\"div\", {\"class\":\"row\"})\n",
        "\n",
        "#info_images = page_blocks[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_cWK3tyfGi2"
      },
      "source": [
        "#After info is downloaded, click \"Next\"\n",
        "#def click_NEXT_on_info_info_page():\n",
        "def click_NEXT_on_info_info_page():\n",
        "  next_button = selenium_webdriver.find_element_by_xpath(\"//button[contains(text(),'Next')]\")\n",
        "  next_button_parent = next_button.find_element_by_xpath('..')\n",
        "  #print(next_button_parent.text)\n",
        "  next_button_url = next_button_parent.get_attribute('href')\n",
        "  #print(next_button)\n",
        "  #print(next_button_url)\n",
        "\n",
        "  next_info_url = selenium_webdriver.get(next_button_url)\n",
        "\n",
        "  uClient = uReq(next_button_url)\n",
        "  page_html = uClient.read()\n",
        "\n",
        "  uClient.close()\n",
        "\n",
        "  info_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  title = info_soup.h4.text.strip()\n",
        "  print(\"Current info page:\")\n",
        "  print(title)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npw0wKq0CtJy"
      },
      "source": [
        "#open subsequent info's URL and print the title\n",
        "def open_subsequent_info_info_pages():\n",
        "  info_url = selenium_webdriver.current_url\n",
        "\n",
        "  #Important!  Any time you get a page for Bs4 using uClient, navigate to that same page using Selenium too!\n",
        "  #otherwise things will get confused.\n",
        "  selenium_webdriver.get(info_url)\n",
        "\n",
        "\n",
        "\n",
        "  uClient = uReq(info_url)\n",
        "  page_html = uClient.read()\n",
        "\n",
        "  uClient.close()\n",
        "\n",
        "  page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "  global title\n",
        "  title = page_soup.h4.text.strip()\n",
        "\n",
        "  #print(subseq_title)\n",
        "  return title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRle8HXbgXCF"
      },
      "source": [
        "#start the download and parse process again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6oKERu9DKtd"
      },
      "source": [
        "def download_entire_set(input_set_url):\n",
        "  uClient = uReq(input_set_url)\n",
        "  page_html = uClient.read()\n",
        "  uClient.close()\n",
        "  info_set_soup = soup(page_html, \"html.parser\")\n",
        "  set_title = info_set_soup.h4.text.strip()\n",
        "  print(set_title)\n",
        "\n",
        "  find_url_for_first_info_info_page_in_set(input_set_url)\n",
        "  #returns first_info_page_url\n",
        "\n",
        "  open_first_info_info_page(find_url_for_first_info_info_page_in_set(input_set_url))\n",
        "  #returns title\n",
        "  \n",
        "  scrape_info_info(title)\n",
        "  #returns desc_block_splitln\n",
        "  #returns title_splitln\n",
        "  \n",
        "  parse_info_info_to_txt(desc_block_splitln,title_splitln)\n",
        "\n",
        "  download_info_images_from_current_page()\n",
        "  \n",
        "  click_NEXT_on_info_info_page()\n",
        "\n",
        "  while True:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "      open_subsequent_info_info_pages()\n",
        "      scrape_info_info(title)\n",
        "    #returns desc_block_splitln\n",
        "    #returns title_splitln\n",
        "      parse_info_info_to_txt(desc_block_splitln,title_splitln)\n",
        "      download_info_images_from_current_page()\n",
        "      click_NEXT_on_info_info_page()\n",
        "      end_time = time.time()\n",
        "      tot_time = end_time - start_time\n",
        "      print(\"info Info Downloaded In: \" + str(tot_time) + \" seconds\")\n",
        "\n",
        "    except:\n",
        "      break\n",
        "      print(\"no more infos in set\")\n",
        "  \n",
        "    #add a function in here that breaks out of the loop if no next info is found\n",
        "    #like a boolean or something\n",
        "  #return to sets page\n",
        "\n",
        "#iterate to next set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRBTS1UCc2Nf"
      },
      "source": [
        "#It looks like the \"scrape subsequent infos\" thing does not include actual scraping and saving :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQshZUpxVw_Z"
      },
      "source": [
        "for by_link in brand_year_urls_all_list[0]:\n",
        "  try:\n",
        "    download_entire_set(by_link)\n",
        "  except:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icsdOPaFsFpH"
      },
      "source": [
        "%cd /category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIu1rtCX_9hZ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cheumA9TDctE"
      },
      "source": [
        "!zip website.zip -r *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNbd0VgJDi53"
      },
      "source": [
        "!rm -r *"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}